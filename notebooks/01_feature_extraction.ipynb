{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50ec87a2",
   "metadata": {},
   "source": [
    "##### Step B.1 --> Importing the required things "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "660b214b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Rohini\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import matplotlib.pyplot as plt \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dfef10",
   "metadata": {},
   "source": [
    "##### Step B.2 --> Loading the CIFAR - 10  dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36fe1341",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rohini\\Desktop\\2 Unsupervised Image Orgniser\\venv\\Lib\\site-packages\\keras\\src\\datasets\\cifar.py:18: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
      "  d = cPickle.load(f, encoding=\"bytes\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "(_, _), (x_test, _) = cifar10.load_data()\n",
    "\n",
    "x_data = x_test[:1000] #using the 1000 images only \n",
    "x_data = x_data.astype(\"float32\")\n",
    "print(x_data.shape)        # (1000, 32, 32, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe94e44",
   "metadata": {},
   "source": [
    "##### Step B.3 --> Resize the images (Critical Step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1bfb8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# basicallt this step is resizing the every image in the dataset to 224 x 224 size, regardless of what size it is \n",
    "\n",
    "x_resized = tf.image.resize(x_data, (224, 224))\n",
    "print(x_resized.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87b08a0",
   "metadata": {},
   "source": [
    "##### Step B.4 --> Load the MobileNetV2 (Feature Extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "434991c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rohini\\AppData\\Local\\Temp\\ipykernel_17560\\2725545370.py:1: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  model = MobileNetV2(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "model = MobileNetV2(\n",
    "    weights = \"imagenet\",\n",
    "    include_top = False,            # this remove the classifier \n",
    "    pooling = \"avg\"                 # This step gives us 12000 vectors, fixed length embedding \n",
    ")\n",
    "\n",
    "model.trainable = False             # interference only "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a89243",
   "metadata": {},
   "source": [
    "##### Step B.5 --> Preprocess & Extract the Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8d520a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 580ms/step\n",
      "Feature Shape : (1000, 1280)\n"
     ]
    }
   ],
   "source": [
    "x_processed = preprocess_input(x_resized)\n",
    "features = model.predict(x_processed, batch_size = 32, verbose = 1)\n",
    "\n",
    "print(\"Feature Shape :\", features.shape)\n",
    "\n",
    "# the output feature shape : is the DNA of the each image  \n",
    "# the 1280-D is a vector "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a31b9e",
   "metadata": {},
   "source": [
    "##### Step B.6 --> Sanity Checks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448fd6ea",
   "metadata": {},
   "source": [
    "##### Check the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff62c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.709743)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(features) \n",
    "\n",
    "# if this is == 0, then something is broken, \n",
    "# if not zero, then the embeddings are meaningful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5426b7",
   "metadata": {},
   "source": [
    "##### Compare the two random images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "099080ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Distance :  33.902473\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import norm \n",
    "a, b = np.random.choice(1000, 2, replace = False)\n",
    "dist = norm(features[a] - features[b])\n",
    "print(\"Feature Distance : \", dist)\n",
    "\n",
    "\n",
    "# different images--> large distance, similar images --> smaller distance \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982e8d8a",
   "metadata": {},
   "source": [
    "##### Save the Articrafts so that we never recompute the embeddngs, streamlit app stays fast & clean seperation of concerns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e6c055d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../app/image_data.npy\", x_data)\n",
    "np.save(\"../app/image_features.npy\", features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Unsupervised Image Organizer)",
   "language": "python",
   "name": "unsupervised-image-organizer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
